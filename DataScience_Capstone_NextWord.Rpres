Predicting Next Word - Capstone Project
========================================================
author: Sergio Vicente  (@svicente99)
date: January 2016
transition: fade
transition-speed: slow

<style type="text/css">
/* slide titles */
.reveal h3 { 
  font-size: 50px; text-decoration: underline;
  color: navy;
}
.noHyphen {
  word-wrap: normal;
  -moz-hyphens: none;
}
/* customizing tables exhibition */
.reveal table {
  border-style: solid;
  border-color: gray;
  width:60%;
}
.reveal table th {
  background-color: #d0d0d0;
  font-size: 14pt;
}
.reveal table td {
  width: 33%;
  font-size: 13pt;
}
</style>

<hr style="border: 0; border-bottom: 2px dashed #ddd; background: #999;">
<h2 style="color:maroon">Data Science Specialization</h2>
<h3>Coursera.org</h3>

Summary
========================================================

<div class="noHyphen"><em>Here we report, in high level, a description of the Capstone Project development, as final product of </em>Coursera<em> Data Science Specialization.</em></div>

<blockquote style="font-family:Arial,sans-serif; font-size:20pt">
The objective of the App is implementing a predictive model that offers hints to one or more words, coherent to the sentence that's been input by its user. The Capstone dataset used includes twitter, news and blogs from HC Corpora. After performing data cleansing, sampling and sub-setting, we gather all data in R data frames. Applying some Text Mining &#40;TM&#41; and NLP techniques, is created some set of word combinations (N-grams). These are the main support to <b>Katz Backoff</b> algorithm predicts the next word. Some adaptations and heuristics were specially developed to enhance this Shiny application.
</blockquote>

How the app works
========================================================
incremental: true

<small>
Just type a word, phrase or sentence. The app shows what the user has entered, followed by cleansed form. As the main result, until the top five (more probable) n-grams predictions are displayed in a list control. The user can review or swap your input data, and the app will turn back to present more hints to predict. Another tab offers a more extensive documentation.
</small>

<h4>Main steps to achieve next word(s) predictions:</h4>

<ol style="font-size:18pt; line-height:20px;">
<li>Loading 4 data frames contained <b>n-grams</b> combinations with 4-words, 3-words, 2-words, and 1-word previously generated.</li>
<li>Reading user input (a word or sentence)</li>
<li>Cleansing of user input (lowering, profanities removing, tokenization of input words: the last four)</li>
<li>Call to prediction model function, basically, the <b><em>Stupid backoff</em> algorithm</b> (a more simplified approach to Katz Backoff):</li>
  <ul style="width:110%">
  <li>search in the <em>fourgram</em> data frame, if found, shows top 5 most probable matches. Otherwise;
  <li>&nbsp;&nbsp;&nbsp;search in the <em>trigram</em> data frame, by the same way above. Otherwise;     
  <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;search in <em>bigram</em> data frame, by the same way above. 
  <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else, at last, if none matching, displays the most frequent words in the <em>unigram</em> data frame.
  </ul>
</ol>

N-grams excerpts
========================================================

<small style="letter-spacing: -0.02em; font-size:18pt;">
See 5 lines of *_"bigrams"_* and *_"trigrams"_* data frames which are loaded by Shiny App.</small>
<p style="margin-bottom: -29px;"></p>

```{r,echo=FALSE}
df3<-readRDS("./Shiny_App/trigrams.rds")
df2<-readRDS("./Shiny_App/bigrams.rds")

library(knitr)
kable(head(df2,5), 'html', table.attr='id="df2_table"')
kable(head(df3,5), 'html', table.attr='id="df3_table"')
```

Viewing the Shiny App 
========================================================

![Shiny App - Next Word Prediction](./Shiny_App/ShinyApp65.png)

<script type="text/javascript">
function improve_text() {
  return "Possible ideas to improve the App:\n\n"+
  "· Use of other techniques to get machine prediction, like "+ "interpolated Kneser-Ney Smoothing, pure Katz Back-Off or Good-Turing discounting;\n\n"+
  "· Adapt to other languages, like Portuguese (my native language), using a Brazilian corpus.";
}
</script>
  
- <a href="https://svicente99.shinyapps.io/NextWord/" target="_blank" style="color:maroon">Link to __Shiny App__</a>&nbsp; &larr;&larr;&nbsp;
*_Use it!_*

- <a href="http://github.com/svicente99/DataScience_Capstone_ShinyApp" target="_blank">__GitHub__ repository code to this application</a>

- <a href="javascript:alert(improve_text())">Possible improvements</a>...
